nmkt6e6d6b74/training-roberta-large-using-cross-entropy
trnkhnhlinh/kali-baseline
vigneshbaskaran/commonlit-nn-kit
bannourchaker/transferlearning-bert-glove-xgb-lstm-attention
abhishek/debertapairwise466
bond005/readability-bert
sahilt93/feature-extraction-bow-word2vec-tfidf-base-model
cdeotte/sentence-compare-approach-0-495
hinepo/clpr-5-models-v1-1
kentaronakanishi/clrp-095-ensemble13-cv-kaggle93
annbogdanova/readability-feature-engineering
romansafronenkov/commonlit-readability
muktan/roberta-large-double-attention-1-2-4-8-16-20
aditidutta/clrp-pytorch-gpt2-implementation
leontrout/fork-of-useful
w5833946/f50-train
bayartsogtya/structbert-train-script
rajat95gupta/clrp-ensemble-3x-inference
kylecloud/commonlit-readability-distilbert-5folds
wuharlem/bert-simple
dijiswiki/clrp-ensemble-models-xgboost-roberta
qinhui1999/readability-roberta-large-infer-pseudo-label
bond005/twin-roberta-regression
davidmezzetti/commonlit-mean-pooled-regression
bacterio/7-entrenamiento-transformer
masteryux/commonlit-nlp-data-augmentation
artificialmeee/readnet
nikitaglazunov/simple-dnn-to-achieve-decent-score
wamateusz/linear-regression
oleksandrsirenko/commonlit-text-cleaning
neel783d/commonlit-regression-model-comparison
artificialmeee/readnet-inference
allohvk/comlit-tryst-with-han
atamazian/clrp-model-with-various-poolers
omkargangan/commonlit-readability-competition
ppjanka/2021-commonlitreadability
bond005/spacy-readability
binhhuunguyen/attention-mean-pooling-roberta
alankadiev/why-am-i-getting-notebook-threw-exception-error
mushaya/readability-index-basic
bannourchaker/commonlit-readability-prize-part1-eda-confirmed
bannourchaker/readability-tfidf-textstatis-count-feature-propipe
shinomoriaoshi/readability-training-notebook
ppjanka/2021-commonlitreadability-final
rajat95gupta/commonlit-ensemble-5x-inference
ruoxijia/clrp-mp-xgb
aleron751/3-roberta-models-blending
shubhamsindal0098/commonlit-readibility-prize
iamshamikb/clrp-eda
nairrahnawarahmed/litmodel
mbmackenzie/feature-engineering-and-tfidf
bannourchaker/commonlit-readability-prize-part2-baseline-bert
himako/allennlp-jsonnet-weak-example-using-roberta
yeahmove/clrp-ensemble-3x-inference
abdelrhmanhosny/baseline-tfidf
raghavendrakotala/pytorch-basline-lstm
trustw/commonlit-02-doc2vec-lgbm-20210723
nitekot/commonlit-readability-lab2
easyjaime/clrp-ensemble-3x-inference
markmario/fork-of-clrp-ensemble-3x-inference
shinomoriaoshi/readability-roberta-large-inference
sharrpie/commonlit-readability-baseline
javi23ruiz/sbert-feature-engineering
ivanajankov/lstm-nlp
leontrout/useful
rajat95gupta/mean-pooling-4-seeds
chamecall/boosting-train
maximkazantsev/commonlit-readability-roberta-lgbm
bond005/ensemble-after-readability-bert
yogesh1190/rating-passage-reading-complexity
yshrhr/clrp-roberta-stacking
madhurrawat/complexity-mr
serkankaanbahsi/attention-bert-gru
iamshamikb/commonlit-4
giangdnghong/mse-k8-group3-final-project
vikasvikkyd/commonlit
surajitcba2021/clrp-gru-withattention
bannourchaker/commonlit-readability-prize-part21-baseline-bert
nafiularaf/commonlit-readability
mabdelhamidmamin/tree-regression-with-dale-table
o00585/rand95
mattjohn/commonlit
nur988/distiilbert-simplelinear-regression-implementation
trustw/commonlit-01-eda-20210723
victororlov/bert-simple-baseline-tensorflow
toutatsu/bert-test
ruriarmandhani/commonlit-submission
atsushiiwasaki/clrp-stratify-on-target-s-e-bins-from-sturge-s
kunihikofurugori/step1-exclude-anomaly
anniedsouza/comlit-lstm
sapal6/commonlit-dataaug
kunihikofurugori/make-nlpdataset
linainversez/bradley-terry
kunihikofurugori/training-commonlit-readability-bertmodel
rajat95gupta/clrp-mean-pooling-inference
markwijkhuizen/simple-normal-wikipedia-sections-pretraining-v1
ligala/readability
kartikbhargav/clrp-roberta-model-and-inference
aniketsharma00411/commonlit-readability-ridge
maximkazantsev/commonlit-readability-univ-sent-encoder-xgb
renjithrrkj/readnet-implemetation
petrand/submission-example
lucamtb/commonlit-transformer-train-and-inference
jacob34/commonlit-create-target-samples
sinatavakolibanizi/readability-feature-engineering-non-nn-baseline
lasaljaywardena/feature-engineering-and-pure-xgb-regressor
legend123345/commonlit-transfer-learning-use
nitekot/notebook3d5cda6fca
legend123345/notebookf0cf8e8b72
pichenguang/fork-of-commonlit
kuroyuli/commonlit-readnet-using-fastai
shinomoriaoshi/readability-roberta-large-oof
davidgzx/stratifiedkfold-on-commonlit-dataset
gaozhao/461yuanshi
lililycai/clrp-pytorch-roberta-pretrain
w5833946/f41-train
w5833946/f32-train
w5833946/f2-train
w5833946/f1-itpt
gilfernandes/commonlit-pytorch-t5-large-svm
yassinealouini/how-to-huggingface
artemglazunov1990/transfer-learning-with-roberta-large
artemglazunov1990/roberta-save
shivarama/word2vec-xgb-starter-kit-101
jcesquiveld/generate-extra-data-with-web-scraping
akashdotcom/commonlit-using-glove-bidirectional-lstm
abhishekb0210/commonlit-eda-initial
ambarish/common-readability-embeddings-lstm
tutty1992/clrp-pytorch-roberta-finetune-roberta-large
yogeshkulkarni/commonlit-readability-prize-with-pytorch
sujithkpanoor/autonlp-finetune-save-weights
sujithkpanoor/required-data
tutty1992/clrp-pytorch-roberta-pretrain-roberta-large
maunish/clrp-pytorch-roberta-finetune-fixed-minor-issues
raghavendrakotala/basiceda
chamecall/clrp-finetune-roberta-large
bacterio/6-nlp-con-transformers
artemglazunov1990/inference-with-roberta
vigneshbaskaran/commonlit-pt-vs-tf
sergeykubarev/commonlit-with-kerasregressor
tbhavnani/bertseq-with-reading-parameters
virajjayant/commonlit-readability-transformer-network
nosilver/roberta-large-base-kfold-train-e35
lucamtb/readibily-nn
yhirakawa/bertopic-visualization-of-topic-modeling
mzntaka0/inference-with-fasttext
yogeshkulkarni/commonlit-readability-prize-with-autokeras
chenhuang666/a-baseline-model-implemented-by-bert
raghavjha858/bert-nlp-eda
calvchen/bert-regression-using-pytorch
komurase/clist-bertvec-lgbm
mcarujo/ease-read-prediction-nlp
sourabh3570/commonliteraturetarget
sergeykubarev/commonlit-readability-simple-models-to-start-with
dlaststark/commonlit-no-fancy-stuff
datajmcn/nlp-read-eda
junjitakeshima/beginner-s-gradual-improvement
mayur7garg/common-lit-readability-similarity
fumitoogasawara/commonlit-bert-lightgbm
artificialmee/clrp-finetune-032fa4
vineethakkinapalli/clrp-roberta-base-representation-techniques
dhruv1234/submit2
bacterio/5-submit-tfidf-ridge
ananduk1993/clrp-data-clean-excerpt-stats
biennh96/commonlit-submission
dhruv1234/submission1
ananduk1993/clrp-feature-engineering-baseline-prediction
azertway/textstat-features-ridge
tracyporter/commonlit-readibility-gaussianprocess
masatomurakawamm/clrp-roberta-simple-finetune-baseline-1
jcesquiveld/best-transformer-representations
panxnan/submittion-test
bacterio/4-regresion-tf-idf
ktamta/eda-baseline
panxnan/notebook-commonlit-readability-basedline
chamecall/pre-trained-roberta-solution-in-pytorch
rajat95gupta/pre-trained-roberta-solution-in-pytorch
rajat95gupta/commonlit-roberta-base-starter-implementation
mdhamani/clrp-roberta-pre-training-using-deepspeed
takamichitoda/commonlit-probing
masatomurakawamm/clrp-customized-robertamodel-fine-tuning
baokar7/clrp-distilrobertabase-training
thegreatjayant/deep-evidential-regression
markwijkhuizen/simple-normal-wikipedia-abstracts-pretraining
lukenguyen/basic-analysis
nullpo20170707/word2vec-lightgbm
takiholadi/02-commonlit-bert-cased-avg-folds
markwijkhuizen/simple-normal-wikipedia-abstract-dataset
muhammadalaref/mha-commonlit-readability-prize
tbhavnani/baseline-bert-regression-using-pytorch
oleksandrsirenko/commonlit-in-depth-eda-baseline
pranjalchatterjee/bag-of-words-word-vectors-commonlit-readability
kushagrgoyal/common-readibility-trial
taherhaggui/roberta-fine-tunning
taherhaggui/roberta-training
taherhaggui/bert-fine-tunning
artificialmeee/pre-trained-roberta-solution-in-pytorch
ktgiahieu/bert-base-readability
afufiza/commonlit-analysis
asvskartheek/bert-tpus-jax-huggingface
chamecall/clrp-pretrain
rowenwitt/rough-spacy-vectors-simple-bidirectional-lstm
mbonnet/distilbert-fine-tuning-data-augmentation
rohitgupta29/basic-modelling-v1
abhishek1aa/stacking-ensemble-roberta-xgboost-ridge
yuichihiguchi/commonlit-readability-prize-eda
chamecall/clrp-finetune-single-roberta-base
sourabh219/roberta-baseline-model
taherhaggui/training-bert-models
ds572370/deep-evidential-regression
julian3833/2-learning-out-of-the-box-roberta-lb-0-53
sambhavsg/clrp-training-and-inference-glove-embeddings
ligtfeather/squeezebert-madgrad-grad-accumulation-w-b
prajittr/commonlit-readability-glove-xgb-nn-rf-ensemble
dsquareindia/roberta-svm-pytorch-lightning
the0electronic0guy/commonlit-roberta-top-12
andretugan/lightweight-roberta-solution-in-pytorch
chamecall/clrp-roberta-finetune
julian3833/3-learning-out-of-the-box-electra-lb-0-58
vigneshmano/conv1d-test-1
sambhavsg/data-understanding-and-baseline-model-clrp
jennyteng/project
shivamanhar/data-extraction-and-data-visualisation
bacterio/3-entrenamiento-regresion-lineal
tusonggao/clrp-pytorch-roberta-base-lightgbm
ankitsinghcoder/fork-of-commonreadability
tusonggao/clrp-pytorch-roberta-base-svm
tusonggao/clrp-roberta-base-my-first-model-5fold-cv
dlaststark/commonlit-bert-variants-p1
chamecall/train-val-split
idwivedi/huggingface-transformers-for-regression-colab
mbonnet/baseline-pretrained-distilbert-and-regression
hinepo/clrp-electra-inference-lb-0-504
dragospancescu/commonlit
sylviacat1222/notebook7fe0fca25f
rhtsingh/swa-apex-amp-interpreting-transformers-in-torch
satyendrapandey16/notebook2bf07f2904
tracyporter/commonlit-readibility-svr
tusonggao/clrp-pytorch-roberta-base-my-first-model-inference
walywaly/readingease
tracyporter/commonlit-readibility-svr-single-y
akabaev/baseline-tf-distilbert
w1023672708/fine-tuning-bert-using-transformers
shahules/guide-pytorch-data-samplers-sequence-bucketing
taherhaggui/inference-roberta-lgbm
rsesha/commonlit-keras-nnlm-tfhub
bacterio/2-bd-ia-directo-env-o-de-referencia
th3niko/keras-bilsmt
dlaststark/commonlit-roberta-variants-p1
bacterio/1-bd-ia-directo-eda
douglaskgaraujo/commonlit-readability-text-features
tatamikenn/rough-argument-of-theoretical-lower-bound
gabellinifrancesco/nlp-from-bag-of-words-to-transformer-commonlit
kumarsrikant/commonlit-readability-nlp-guide-for-beginners
bharadwajvedula/clr-lb-0-475-lazy-way-to-get-good-score
yhirakawa/textstat-how-to-evaluate-readability
weka511/commonlit-variability-of-bradley-terry
ruriarmandhani/clrp-roberta-lstm
drzhuzhe/roberta-base-with-radam-lookahead
socathie/basic-centroid-w-fasttext
haiyunhu/commonlitreadabilityprize-train
krishna1997gopal/understand-roberta
osblouf/external-data-for-model-pretraining
evansimpson/eda-for-commonlit-reading-prize
bumjunkoo/commonlit-run-glue
vermichel/commonlit-readability-prize
sergioli212/tutorial-text-processing-using-python-and-nltk
samarthsharma1408/commonlit-readability-price-simple-eda
w1023672708/tfidf-xgboost
adityabhat/learn-nlp-through-commonlit-readability
andretugan/pre-trained-roberta-solution-in-pytorch
b0721227/clrp-pytorch-roberta-finetune-tpu
kirichenko17roman/dl-lab-17-lstm-for-nlp
kirichenko17roman/dl-lab-15-feature-engineering-in-nlp
reinaueda/submit
vigneshbaskaran/commonlit-halftime-recap-of-my-transformer-journey
emrearuk/commonlitreadabilityprize
andretugan/commonlit-two-models
amiteshgangrade/gradient-boosting-with-glove-embedding
vigneshbaskaran/commonlit-making-my-transformer-good-enough
kirichenko17roman/hanet-regression
ashokkumarbibbab/commonlit-redabilitytest
harshmishraandheri/submission-csv
tarandeep97/performance-comparison-of-regression-models
ttjccc/pytorch-1-9-0
mikelovesrobots/substantial-book-club-readability-prize
syurenuko/clrp-word2vec-lightgbm-baseline
katsuyanomura/word2vec-lgbm-baseline
biswajitroy7890/bert-embeddings-xgboost-regression-text-data
ashokkumarpalivela/keras-transformers-from-scratch-5-fold-cv
moeinshariatnia/commonlit-easy-data-augmentation
pradipkumardas/4-commonlit-readability-simple-roberta-with-cv
andreychubin/commonlitreadabilityprize
elenakhachatryan/commonlit-readability-prize-eda-baseline
craving1030/caret-and-simple-text-fe-of-commonlit-readability
drzhuzhe/commonlit-stratified-kfold
vijendersingh412/hugging-face-model-roberta-large-with-svm
shivarama/samplebert
annazhukovets/read-rate-svr
vijendersingh412/classification-with-tensorflow-and-use-large
hwangdongjun/predict-readability
eiann1509/commonlit-fine-tuning-with-roberta-base
aniketsharma00411/commonlit-readability-data-observations
aniketsharma00411/commonlit-readability-svr
aniketsharma00411/commonlit-readability-random-forest
kazu02/how-to-search-optimum-hyperparmeter-use-wandb
medvedew/commonlit-readability-fasttext-lstm-two-loss
wanko123/bert-with-targets-and-errors-using-trained-outside
masatoootsuka/commonlit-eda
maunish/clrp-pytorch-roberta-finetune-tpu
alphadraco/commonlit-readability-prize-roberta-torch-itpt
thedrcat/commonlit-readability-feature-importances
srikantakumarsahoo/notebook2f62e91059
kirichenko17roman/dl-lab-14-sequence-classification-regression
dhlongle/basic-nlp-with-random-forest
lillymottershead/commonlit-readability-analysis
iska47/commonlit-data-exploration-and-classic-algorithms
salikhussaini49/text-difficulty-prediction
jamesmcguigan/commonlit-xgbooost
hjhgjghhg/commonlit-readability-roberta-pretrain
vigneshbaskaran/commonlit-easy-finetuner-inference
thedrcat/commonlit-25-distilbert-models
pradipkumardas/3-commonlit-readability-simple-model-with-bert
nikolayromanenkov/lightautoml-roberta
katsuyanomura/eda-crp
takamichitoda/commonlit-classical-methods-for-text-readability
ankarsingh/commonlit-text-cleaning-and-huggingface-model
chamecall/clrp-inference
krishnagk/notebookcommonlitlinearregression
thedrcat/commonlit-hf-minimalistic-example
pradipkumardas/2-commonlit-readability-baseline-perf-1dconvnet
thedrcat/commonlit-hf-trainer-hyperparameter-tuning
douglaskgaraujo/sentence-complexity-comparison-dataset
krishna1997gopal/understand-bert-in-depth-theory-implementation
gabrielchapeu/gabriel-previs-o
mrruslionkz/notebookff02ed0974
vigneshbaskaran/imdb-simple-bert-finetuner
lucamtb/readability-feature-engineering-non-nn-baseline
adarshsng/complexity-word2vec-pca-conv1d
dlaststark/commonlit-data-augmentation
raymishra/commonlit-building-from-the-base
pradipkumardas/1-commonlit-readability-eda
chamecall/bert-train
qinhui1999/training-tf-roberta-large-on-mlm
bryanb/first-approach-using-xgboost-with-hyperopt
gilfernandes/commonlit-pytorch-distilbart-svm
soumochatterjee/discover-difficulty-with-roberta
maunish/clrp-pytorch-roberta-finetune
solorzano/clrp-roberta-ridge
soumochatterjee/discover-difficulty-with-bert2
adarshsng/complexity-cleaning-f-engg-bi-lstm
sergioli212/clrp-try-allennlp-api-for-plm
thedrcat/commonlit-1-generate-pictures-really
elemento/litread-ml
thedrcat/commonlit-2-computer-vision-approach-really
mpwolke/dataprep-clean-literature
wanko123/simple-bert-with-targets-and-errors-2
ejmejm/commonlit-bert-submission-video-tutorial
ejmejm/commonlit-eda-video-tutorial
gabrielcf8/commonlit-readability-competition
chumajin/bert-v-s-roberta-english
matinroom/fork-of-ensem
mohammedfahadvp/commonlit-readability-prize-beginners-eda
thedrcat/commonlit-train-average-baseline
donkeys/pip-install-package-from-dataset-no-internet
aniketsharma00411/random-forest
rubiales/scraping-data-augmentation
rhtsingh/on-stability-of-few-sample-transformer-fine-tuning
pehahn/basic-readabil
hazigin/commonlit-readability-bert-lightgbm
kanbehmw/pytorch-weebit-dataset-fine-tuning-train-kfold
teeyee314/readability-url-scrape
weka511/list-spacy-tags
pehahn/basic-bert-with-r
grafael/beyond-the-excerpt
konradb/linear-av
teeyee314/readability-external-data-eda
stpeteishii/commonlit-readability-keras-conv1d
stpeteishii/commonlit-readability-keras-bidirectional
ttjccc/download-and-offline-install-the-latest-pytorch
artemzapara/commonlit-readability-prize-eda
eneszvo/bert-roberta-distilbert-ensemble-5-fold-cv
chumajin/how-to-initialize-the-code-correctly-english
johnhawkins/texturizer-lightgbm
getitdone/commonlit-word-to-vec-with-umap
ttjccc/offline-install-pytorch-1-8-1
meihanw/commonlit-autokeras-try-out
weka511/commonlit-readability-prize-bradley-terry
shreyasajal/pytorch-lightning-bert-tpu-weights-biases-logs
bharadwajvedula/tpu-high-speed-roberta-training
smsrikanthreddy/commonlit-non-transformers-models
tpothjuan/basic-eda-and-simple-lstm-model-prediction-tf
hotsonhonet/inference-clrf
stpeteishii/commonlit-readability-bert-xgboost
talha1503/commonlitreadability-lstm-inference
sounaklahiri01/readability-baseline-distilbert
fabriciotorquato/commonlit-readability-prize
evilmage93/commonlit-bert-tuned-svr
solorzano/comparison-of-huggingface-text-model-features
smsrikanthreddy/commonlit-eda-ml-baseline
andradaolteanu/ii-commonlit-bert-vs-roberta-w-b-testing
shanelindh/basic-eda-of-commonlitreadbility-data
wmanka/roll-your-own-rnn-pytorch
thecurvefitter/roberta-embedding-ml
redandhey/fork-of-fork-of-two-roberta-s-are-better-than-one
thecurvefitter/roberta-model
samsonteo/common-readibility001
accountstatus/readability-index-in-python
olivrk/simple-ensemble-metalearner
rerere/commonlit-readability-prize-lstm-approach-tf-idf
rhtsingh/utilizing-transformer-representations-efficiently
shashwatwork/commonlit-the-topic-modelling-approach-with-py
gcmadhan/commonlit-readability-eda-modeling
hiroshihiroshi/sort-your-books-by-genre-topicmodel-r
ajax0564/training-tf-roberta-on-mlm
atsushiiwasaki/commonlit-bert-stratified-k-fold-baseline-train
rerere/commonlit-readability-prize-tf-idf-svm
sourabhy/elmo-svm
sourabhy/generating-elmo-embeddings
atsushiiwasaki/commonlit-bert-stratified-k-fold-baseline-infer
dagnelies/kiss-roberta
deb009/commonlit-readability-prize-using-bert
jnegrini/commonlit-readability-nlp-eda-and-initial-model
narendra/commonlit-glove-gru
aniketsharma00411/commonlit-readability-decision-tree
atsushiiwasaki/how-many-samples-do-i-want
atsushiiwasaki/sample-size-experiment-training-part
maunish/clrp-pytorch-roberta-pretrain
rhtsingh/two-roberta-s-are-better-than-one-0-469
sourabhy/commonlit-roberta-ensemble-multichannel-cnn
wanko123/roberta-cnn-with-target-and-error
shrutisaxena/commonlit-using-h2o-flow
vigneshbaskaran/commonlit-text-comparator-rmse-vs-ranking-loss
getitdone/beginners-friendly-notebook
donmarch14/commonlit-detailed-guide-to-learn-nlp
sumantindurkhya/commonlit-baseline-doc2vec
rhtsingh/commonlit-readability-prize-roberta-torch-infer-2
shreyasajal/prompt-tuning-bert-commonlit-readability
shaz13/code-how-bradley-terry-model-works
duttadebadri/eda-basline-modeling-commonlit-passages
drdevasiakurian/simple-commonlitreadability-for-beginners
sumantindurkhya/bert-for-regression
talha1503/commonlitreadability-eda
shinomoriaoshi/readability-data-augmentation
felixneate/commonlit-signalvsnoise
kozkiuay/pytorch-bert-for-commonlit-readability
sumantindurkhya/commonlit-baseline-tf-idf
ashutoshiitg/clrp-roberta-train
tomwarrens/commonlit-readability-prize-eda
revathiprakash/commonlit-readability-eda-kit-wip
iammir/commonlit-distil-roberta
pehahn/textfeatures-in-r
hiroshihiroshi/word-length-and-tf-idf-has-negative-correlation
vigneshbaskaran/commonlit-pytorch-vs-sklearn-regression
lautenschlager/glove-preprocessing-feel-free-to-use
lekynam2000/bart-pretrained-no-fold
mdhamani/clrp-roberta-train
lautenschlager/readability-baseline-using-simple-stats
thedrcat/commonlit-what-are-we-reading-about
shaz13/augment-at-word-level-and-help-your-nlp-model
indranilbhattacharya/tsne-of-bert-embeddings-3d-plots
shaz13/using-commonlit-augmented-data-set
lucabottero/statistical-features-from-text
narendra/commonlit-word-proportion-lda-baseline
wanko123/simple-bert-cnn-with-targets-and-errors
dtomruk/commonlit-eda-modeling
jeongbinpark/using-subwordtextencoder
bhaveshkumar2806/commonlit-readability-basic-eda-and-roberta-base
vigneshbaskaran/commonlit-spacy-with-ridge-regression
tommasomarzi/sub-scoring-error
bharadwajvedula/lb-0-683-tf-glove840b
pavelpds1/readability-fcnn-embedding
evilmage93/common-lit-bilstm-based-regression
vigneshbaskaran/commonlit-a1-pure-scikit-learn
abhilashreddyy/0-6-rmse-finetuning-transformer-for-dummies
jeongbinpark/using-keras-word-embedding
rhtsingh/guide-to-huggingface-schedulers-differential-lrs
shaz13/commonlit-basic-feature-engineering-on-raw-text
jeongbinpark/using-word2vec
simoneazeglio/word2vec-umap-w2vaugmentation
duboisian/first-draft-model
xinruizhan/pretrained-bert-nn
rhtsingh/speeding-up-transformer-w-optimization-strategies
rohitpande/feature-engg-with-basic-linear-regression
lekynam2000/bart-custom-loss-use-stderror
lars123/neural-tangent-kernel
crained/prep-data-with-keras-for-commonlit
krishna1997gopal/important-in-readability-eda-tf-idf-w2v-xg-boost
pavelvod/transformer-cnn-lstm-attention-head-inference
pavelvod/transformer-cnn-lstm-attention-heads
superbb666/superbb666-ngram
crained/fun-with-commonlit-using-scikit-learn-nda
prvnkmr/tf-bert-baseline-lb-0-646-inference
prvnkmr/tf-bert-baseline-lb-0-646-training
tealgreen0503/the-simplest-stratifiedkfold-for-regression
thomaskonstantin/commonlit-feature-engineering-age-range-bert
kunalman/clrc-eda-v0-1
maostack/clrp-how-to-get-text-embedding-from-roberta
prvnkmr/domain-knowledge-readability-score-methods
kumapo/bert-baseline-training-validation
rhtsingh/commonlit-readability-prize-roberta-torch-infer-3
crained/fun-with-commonlit-nltk-nda
takusid/popular-readability-formulas-and-metrics
geoffcc/notebook935a1345d7
rushinaik/bigwords
pehahn/dnn-tokenizer
ttjccc/gaussian-multi-target-regularization
pranav082001/commonlit-pytorch-bert-starter
mghfarahani/commonlit-readability-prize-analysis-prediction
sourabhy/commonlit-roberta-cnn
abee82/commonlit-readability-using-fastai-v2
hiroshihiroshi/low-freq-low-target-and-texteda-with-r
crained/deberta-pytorch-commonlit-readability-train
shreyasajal/pytorch-openai-gpt2-commonlit-readability
trantrikien239/preprocessing-and-svr-baseline
threesheds/simple-bow-baseline
pawan2905/commonlit-readability-prize
shreyasajal/pytorch-bert-baseline-lr-schedulers-guide
ash112/commonlit-readability-gbr
mehrankamal/commonlit-roberta-cnn
atharvap329/torchflare-easy-model-training-bert-baseline
jollibobert/commonlit-readability-roberta-xgb-baseline
mehrankamal/fork-of-commonlit-roberta-cnn
crained/deberta-pytorch-inference
rushikeshdarge/easiest-approach-with-good-rmse
ochaaaaaaan/commonlit-doc2vec-lightgbm
jollibobert/commonlit-simple-se-and-excerpt-length-eda
abhilashreddyy/a-deeper-eda-on-pos-tags-topic-modelling-more
simakov/lama-bert-inference
simakov/lama-bert-starter
ayuraj/how-to-use-tf-data-to-train-hf-transformer
subinium/commonlit-how-to-visualize-text-dataset
rushikeshdarge/commonlit-using-neural-network
virksaab/data-augmentation-and-correlation
yujikawa/commonlit-readability-prize-eda
gokhankesler/best-i-can-do-with-sklearn
yeayates21/commonlit-tf-gpu-xlm-roberta-ssl
solverworld/how-big-are-you-test-set
chumajin/inference-for-pytorch-bert-beginner-s-room
ragnar123/commonlit-readability-roberta-tf-inference
sauravmaheshkar/commonlit-tensorflow-weights-biases
weka511/explore-syllables
magdawjcicka/commonlit-readability-data-exploration-nn
yeayates21/train-commonlit-xgb-baseline-advanced-features
harshsharma511/pytorch-lightining-roberta
konumaru/baseline-with-linear-regression
ragnar123/commonlit-readability-roberta-tf
amontgomerie/baseline-with-features-from-textstat-and-spacy
praveengovi/commonlit-bert-roberta-model-explainablity
abhishek/fork-of-fork-of-yum-yum-yum-93f968
weka511/naive-readability
msafi04/tensorflow-roberta-commonlit-readability
rhtsingh/commonlit-readability-prize-roberta-torch-fit
darknesszx/roberta-simple-roberta-large-baseline
sumeetsawant/commonlit-readability-ridge-baseline-0-7
muhammad4hmed/commonlit-hierarchical-attention-networks
rahatreza/simple-randomforest-pipeline
kazu02/commonlitreadabilityprize-model1
ravillatejakumar/fast-ml
cascadinglight/commonlit-dimensionality-reduction-and-eda
andreshg/commonlit-a-complete-analysis
thomaskonstantin/commonlit-text-analysis-baseline-modeling
jeongyoonlee/tf-keras-bart-baseline-training-inference
zefirchik/start-bert-models
ankur310794/crp-mix-6-models-inference
srijita97/basic-feature
harshsharma511/one-stop-understanding-eda-bert-baseline
maksymshkliarevskyi/commlit-readability-start-with-r-eda-nlp-lstm
narendra/basic-eda-tabular-baseline-pytorch-nn
jeongyoonlee/pytorch-lightning-roberta-training-inference
debarshichanda/explore-t5
deb009/commonlit-readability-prize-eda
bootiu/commonlit-pytorch-lightning-bert-benchmark
pranav082001/commonlit-simple-ensemble-models-ml
ammarnassanalhajali/commonlit-readability-eda
raoumair98/bogog
viethoang303/commonlit-readability-prize
sharrpie/commonlit-readability-eda-fe-topic-modelling
manabendrarout/eda-bow-classical-model-with-autokeras-ensemble
houssemayed/lstm-models-for-regression-on-text-data
wanko123/exploring-features-and-model-building
mzntaka0/simple-vectorization-using-doc2vec
yuto16/grammar-spell-checker
udbhavpangotra/commonlit-readibility-prize-extensive-eda-model
rhtsingh/commonlit-readability-prize-roberta-torch-itpt
gogo827jz/roberta-model-parallel-fold-training-on-tpu
sourabhy/common-lit-cnn-model
pehahn/tidymodels-nb3train
harshitkhandelwal/commonlit-eda-literature-submission
kneroma/clr-prize-roberta-model-training
utcarshagrawal/commonlit-eda-most-nlp-techniques
aruthart/transformers-fastai-starter-submission
prajittr/commonlit-dnn-randomforest-linear-regression
barun2104/commonlit-readability-eda-svd-modeling
rybalkin/basic-readability-metrics
abhibisht89/using-textstat-eda-and-readability-scores
vislupus/nlp-commonlit-readability
riteshpatil8998/commonlit-readability-cross-validation
jiny333/creating-whl-files-to-install-external-libraries
jeongyoonlee/all-zero-submission
indranilbhattacharya/eda-feature-engineering
kritanjalijain/commonlit-eda-and-lstm
anubhav1302/simplemodel-tf
manishkc06/text-pre-processing-data-wrangling
abdokamr/starter-eda-baseline-ridge-and-xgboost
hengzheng/simpletransformers-regression-starter-less-code
jiny333/simple-baseline-using-pycaret-training-inference
aruthart/transformers-fastai-quick-starter
abhibasavapattana/tensorflow-glove6b-lit-readability
pashupatigupta/commonlit-baseline-using-textstat
houssemayed/exploratory-data-analysis
kshitijmohan/commonlit-readability-bert-train-inference
srijita97/eda-work
heyytanay/submission-pytorch-bert-kfolds-inference
yuto16/target-standard-error
ajain3982/commonlit-readability-skipgrams-linearreg
vaibhavsxn/bert-fine-tuning-sentence-scoring
gustavomodelli/readability-keras-model
tchaye59/clreadability-tf-bert-baseline
gustavomodelli/readability-lasso-regression
hamditarek/commonlit-readability-prize-lsa-lda-nn-baseline
anandsm7/commonlit-roberta-training-inference
vivekprajapati2048/a-complete-commonlit-readability-prize-nb
tchaye59/download-tensorflow-text-tf-models-official
yeayates21/commonlit-text-augmentation-eng-to-fre-to-eng
shams1/explor
tobiasgabrielkroll/readability
alaasedeeq/commonlit-readability-eda
dearsijie/commonlit-readability-tf-lstm-tune-hyperparameters
leighplt/simple-recurrent-model-pytorch
kneroma/clr-prize-roberta-model-inference
snnclsr/deadly-simple-baseline
mpwolke/uncommon-literature
shivanandmn/bert-pytorch-commonlit-readability-simple
snnclsr/commonlit-fastai-training-with-language-model
pehahn/r-submission-1
jitshil143/submission-score-0-62
safavieh/basic-word-based-model
ulrich07/ridge-regression-starter
snnclsr/commonlit-pytorch-distilbert-training
nicolasalerno/smote-classifiers-dt-rf-xgb-stacking-ensemble
msafi04/commonlit-readability-bert-baseline-tf
semyonkoshkarov/tf-idf-linearsvr-baseline
anaverageengineer/comlrp-baseline-for-complete-beginners
debarshichanda/pytorch-commonlit-readability-bert-baseline
snnclsr/commonlit-pytorch-distilbert-inference
tungmphung/commonlit-readability-eda
louise2001/rapids-sentenceembeddings-for-regression
aninda/commonlit-fastai
anubhav1302/readability-eda
louise2001/nltk-commonlit-starter-notebook
shams1/prize
chumajin/pytorch-bert-beginner-s-room
praveengovi/commonlitreadabilityprize-all-in-one-eda
hamzaghanmi/crp-eda-preprocessing-lightgbm
ayushggarg/commonlit-readability-prize-eda-and-baseline
josephassaker/commonlit-readability-eda-na-ve-submission
rushinaik/base-model
yeayates21/commonlit-random-forest-na-ve-baseline
heyytanay/auto-keras-text-regression-starter
pavelvod/simple-starter-baseline
roguecomputer/commonlit-readability-eda-plotly-word-cloud
shatakshiraman/commnlit-eda
hayahiko/ridge-10folds-step-by-step
abhishekvermasg1/eda-what-do-we-teach-our-successors-v3
docxian/commonlit-first-glance
pavelvod/catboost-text-features
hamasho/normalizing-and-wordcloud
konradb/linear-baseline-with-cv
hannes82/commonlit-readability-roberta-simple-baseline
abhishek/yum-yum-yum
nrottam/commonlit-readability-prize
lawrencechernin/ridge-regression-starter-with-stop-words
tanmay17061/feature-eda
leighplt/transformers-cv-train-inference-pytorch
hannes82/commonlit-readability-roberta-inference
ravishah1/readability-feature-engineering-non-nn-baseline
rinabuoy/notebookeee3c8f8a2
jeongyoonlee/tf-keras-bert-baseline-training-inference
andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model
abhishek/step-1-create-folds
chumajin/pytorch-bert-beginner-s-room-version
heyytanay/training-kfolds-pytorch-bert-large-w-o-oom
dimitreoliveira/commonlit-readability-eda-roberta-tf-baseline
aishwarya2490/commonlit-readability-prize-eda-cleaning-data
ruchi798/commonlit-readability-prize-eda-baseline
alincijov/nlp-starter-logsoftmax-nlloss-cross-entropy
reighns/general-learning-linear-regression-from-scratch
lqtuantk97/group-4-fsb-lasso
steubk/clrp-benchmark-your-model-with-onestopenglish
ptrikp/distillbert-bert-and-lgbm
shinomoriaoshi/readability-model-architecture
vlomme/commonlit-deberta-large
mananjhaveri/rank-88-clrp-lb-0-457-solution
takoihiraokazu/final-sub1
nyleve/3-roberta-electra-deberta-4-layer
ananduk1993/ensemble-top-models
liuyuanjiang/test-model
yunfenghu/30randversion0
o00585/1007rand
kurupical/191-192-202-228-251-253-268-288-278-final
pichenguang/506rand
gaozhao/42rand
yshrhr/clrp-roberta-xgboost-optuna
sharrpie/clrp-inference-roberta-base
authman/clit-fails
sharrpie/clrp-finetune-roberta-base
darrentitor/0801-g-ensemble
kaerunantoka/commonlit-2nd-stacking-57
sharrpie/transformers-clrp-pretrain-roberta-base
sapal6/commonlit-inf-fwd-student
sagol79/sent-transformer-simple-normal-wiki-texts-svr
sapal6/common-lit-fwd-training-on-pseudo-l
kunihikofurugori/readability-finalsub-model2
kunihikofurugori/readability-finalsub-model1
maheller/universal-sentence-model-5-fold
yunfenghu/version0
w5833946/f-inference-3-1-v4
darrentitor/0731-e-stratified-large-roberta
horsek/clrp-roberta-large-v31s-training-on-kaggle
dhirenkakkar/custom-target-bootstrapping
pichenguang/nepoch2
sapal6/commonlit-inf-ensmble-fwd-bkwd-student
tanesan/use-masked-language-model-scoring
atsushiiwasaki/clrp-standard-error-feature-eda
sapal6/common-lit-fwd-bkwd-training-on-pseudo-label
atsushiiwasaki/clrp-standard-error-as-a-feature
w5833946/f51-train
sapal6/commonlit-pseudo-labeler
w5833946/f-inference-1-1
ananduk1993/cl-roberta-large-lightgbm-better-finetune
vungocbinh/commonlitreadabilityprize-bert-autokeras
legend123345/bidirectional-lstm-spacy-embeddings
jkstarc/commonlit-readability-toeic
sapal6/commonlit-inference-enseamble-fwd-bkwd-lm
atsushiiwasaki/clrp-electra-large-att-conv1d-infer
artemglazunov1990/all-models-inference
andretugan/commonlit-three-public-models
zmax505/notebook20b449257a
takeshikobayashi/bronze-medal-solution-roberta-stacking-ensemble
artemglazunov1990/roberta-base-mp-xgb-5-folds-training-skf
artemglazunov1990/roberta-base-svr-5-folds-training-skf
pichenguang/commonlit-two-models-with-new-model
narendra/clrp-roberta-task-finetune-2
aleron751/lama-bert-inference
aleron751/lama-starter
lqtuantk97/group-4
joeyaramouni/roberta-with-detailed-analysis-and-visualization
chamecall/clrp-inference-with-handcrafted-features
millak/bert-in-depth-understanding
binhhuunguyen/pytorch-roberta-meanpooling-attention
atsushiiwasaki/clrp-roberta-large-att-mask-act-infer
shinomoriaoshi/readability-meta-ensemble-inference
o00585/464-pre6
w5833946/f-inference-1
gaozhao/huantou464
rohithansdah/reading-submission
joezzb/synonyms
gaozhao/prefine1int
juanscanlan/commonlit-inference
tutty1992/clrp-roberta-svm-roberta-large
sujithkpanoor/roberta-finetune-046fb7
sapal6/commonlit-train-lm-with-cbt-data-without-wndb
nicolesy/clrp-inference-b54382
aoduozhang/clrp-pytorch-roberta-finetune
shinomoriaoshi/readability-stacking-inference
gilfernandes/commonlit-pytorch-ensemble-large
ktgiahieu/commonlit-ensemble-distilrobertav3-v6
konstantinitsi/roberta-l-v3
aoduozhang/clrp-pytorch-roberta-inference
atsushiiwasaki/clrp-stratify-on-predictability
tutty1992/clrp-pytorch-roberta-inference-rbt-l-rbt-b-tpu
w5833946/f23-train
sapal6/commonlit-infusing-data-with-same-domain-data
chenfeng1271/fork-of-fork-of-yum-yum-yum-93f968
atsushiiwasaki/clrp-roberta-base-oof-task-fineturning-infer
kaerunantoka/commonlit-110-inf
teeyee314/clr-url-legal-probe
pranonraian/clrp-one-roberta-model
konstantinitsi/bert2-submit1
vivek61/simple-bert-without-finetuning-commonlit
nicolesy/clrp-inference
sapal6/common-lit-forward-backward-training-on-ulmfit
w5833946/f17-train
afgawe/notebook6bff0b0cb5
vineethakkinapalli/commonlit-two-models
atsushiiwasaki/split-based-on-target-standard-error
hosstell/notebookcf69c4ba48
finlay/fit-fgsm
narendra/clrp-hirearchial-attention
chtalhaanwar/simplest-solution
vungocbinh/commonlitreadabilityprize-bert
mzntaka0/document-represenataion-with-fasttext
artificialmeee/notebook705621c585
takiholadi/01-commonlit-linreg-4-features
justinchae/tune-roberta-pytorch-lightning-optuna
narendra/commonlit-gru-with-attention-aggregation
ktgiahieu/commonlit-infer-roberta-base-regression
sujithkpanoor/commonlit-readability-prize-roberta-torch-fit
kunihikofurugori/step2-1-tpu8foldtraining-rbase
narendra/commonlit-readability-prize-bert-finetuning
chamecall/clrp-roberta-pretrain
julian3833/1-learning-out-of-the-box-bert-lb-0-577
taherhaggui/bert-forecasting
chenfeng1271/commonlit-readability-tensorflow-torch-ensemble
aidenkim/commonlit-fastai-baseline
sujithkpanoor/roberta-svm-34
heyangk/kkkkk
nageen94/commonlit-stacking-regressors
kunihikofurugori/tpu8fold-distilroberta
heyanghuang/baseline-hhy-1-0
dev1ceqqq/deberta-oof
adityagarg2809/using-basic-text-features-to-predict-readability
tusonggao/clrp-pytorch-roberta-base-my-first-model-train
kappawarrior/bertforsequence-with-kfold
dev1ceqqq/ensemblethreemethods
b0721129/commonlit-readability-analysis
luanvietnguyen/gpt2-surprisal
dev1ceqqq/crl-newroberta-large-finetune
mananjhaveri/transformers-for-the-first-time
viteshkakadia/performance-comparison-of-regression-models
drzhuzhe/error-analysis
ashuto7h/commonlit-readability2-model
drzhuzhe/crp-roberta-base-tpt
vigneshbaskaran/commonlit-inference-is-my-transformer-good
mobassir/commonlit-readability-tensorflow-torch-ensemble
bumjunkoo/commonlit-run-glue-inference
shivarama/inferencebert
jcesquiveld/roberta-large-5-fold-single-model-meanpooling
gavinjang/make-prediction
justinchae/crp-regression-with-roberta-and-lightgbm
bob2ross/fork-of-commonlit-itpt-weight-decay005-roberta-to
brucebatmanwayne/clrp-distilbert-inference
kunihikofurugori/tpu8fold-transformer-attention
vigneshbaskaran/commonlit-why-my-transformer-is-not-good-enough
vigneshbaskaran/commonlit-easy-transformer-finetuner
wecdecqw/commonlith-bert
rungogo/fork-of-commonlit-readability-prize-rober-itpt
hinepo/commonlit-textstat-feature-eng-xgb-importances
nityasevak/heirarchial-attention-regression
rungogo/fork-of-commonlit-readability-prize-roberta-torc
rerere/commonlit-readability-prize-roberta-approach
kunihikofurugori/tpu8fold-transformer
lhagiimn/clrp-ensemble-public-notebooks
mobassir/readability-tensorflow-roberta-training-baseline
moeinshariatnia/does-augmentation-hurt-let-s-ask-the-data-w-tsne
dsquareindia/commonlit-simple-pytorch-lightning
maunish/clrp-pytorch-roberta-inference
bongbu/notebook41b8773172
nityasevak/a-neural-factorization-machine-approach
dlaststark/commonlit-glove-models-ensemble
terrorblade1/bert-lgb-whynotwork
rolando222/fork-of-ensem
coldog/notebook4submit
thedrcat/commonlit-3-computer-vision-inference
terrorblade1/bertmo
shloksah/roberta-embedding-bayesian-ridge-using-tf
hedwig100/bert-simple-baseline-inference
willrice/commonlit-bert-stratified-k-fold-baseline-train
hedwig100/bert-simple-baseline
yus002/readability-will-i-overfit
taherhaggui/eda-modeling-very-interesting
ankitp013/exploratory-data-analysis-lstm-gru
dsquareindia/sbert-lstm
pppass/commonlit-readability-robertalarge-tf-stratifie
mokicheese/fork-of-fork-of-transformers-cv-train-infe-b02165
soumochatterjee/discover-difficulty-with-xlnet
kunihikofurugori/finetunning-commonlit-readability-transformer
jinhangjiang/commonlit-readability-sbert-xgb
shauryajain/lgbm-regressor
hotsonhonet/helpme
andyanybody/fork-of-commonlit-readability-prize-roberta-torch
soumochatterjee/discover-difficulty-with-distill-bert
nityasevak/ncr-roberta-minilm-mpnet-xgboost
sapal6/common-lit-training
narendra/commonlit-gru-tabular-submision
vigneshbaskaran/commonlit-derive-more-features-from-transformer
joshualancaster/commonlit-readability-prize-roberta-torch-itpt
vigneshbaskaran/commonlit-sentencetransformer-sklearn-regression
matthewfranglen/kaggle-workshop-script
narendra/commonlit-glove-gru-submission
nityasevak/a-zero-feature-engineering-approach
narendra/commonlit-mask-rare-word-model-glove-submission
garuna1234/notebooka455286c9a
sergioli212/clrp-pytorch-train-clean
bruteforcefitting/transformer-cla-xlnet
mehrankamal/transformer-cnn-lstm-attention-heads-distil
mehrankamal/transformer-cnn-lstm-attention-heads-roberta
narendra/commonlit-masking-word-submission
urstrulysai/ensemble-just-do-it
mdhamani/clrp-roberta-inference
rhtsingh/commonlit-readability-prize-roberta-torch-infer
hotsonhonet/training-1
lars123/neural-tangent-kernel-2
abhilashreddyy/inference-transformer-model-using-hugging-face
wptouxx/clrp-xgboost
thecurvefitter/baseline-model
bruteforcefitting/roberta-squad
vdefont/dummy
zfatgxu/notebook214aa91702
hantianluo/huggingface-autonlp
davidjlochner/commonlit-elastic-net
konradb/data-augmentation-shallow
maunish/clrp-roberta-svm
shahules/commonlit-roberta-xgb-ensemble
maunish/clrp-pytorch-train-tpu
yasserhessein/commonlit-eda-gru-glove-840b-300d
konradb/diy-data-augmentation-with-gpt2
aristotelisch/basic-text-processing-lda-roberta-transformer
mdfahimreshm/bert-in-depth-understanding
infernape/textstat-linearregression
datafan07/eda-simple-bayesian-ridge-with-sentence-embeddings
ajain3982/universal-sentence-encoder-nn-using-keras
tchaye59/tfhub-berts-download
gunesevitan/commonlit-readability-prize-eda
gaetanlopez/pytorch-roberta-inference
dpaluszk/commonlit-cannot-submit-submission-csv-not-found
snnclsr/commonlit-fastai-inference
alvarofbudria/commonlit-readability
jinhangjiang/commonlit-readability-sbert-dl-jiang
heyytanay/commonlit-eda-understanding-the-competition
